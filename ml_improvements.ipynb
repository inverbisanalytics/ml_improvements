{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"data/data_health/trace_activities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=[\"index\", \"EVENTID\"])\n",
    "data[\"start\"] = pd.to_datetime(data[\"start\"])\n",
    "data[\"end\"] = pd.to_datetime(data[\"end\"])\n",
    "\n",
    "n_unique_activities = len(data[\"activity\"].unique()) + 1\n",
    "\n",
    "attributes = [\n",
    "    attr\n",
    "    for attr in data.select_dtypes(include=[\"object\", \"bool\", \"number\"]).columns\n",
    "    if attr not in [\"traceId\", \"activity\", \"start\", \"end\"]\n",
    "]\n",
    "\n",
    "\n",
    "def is_trace_level(attribute):\n",
    "    return data.groupby(\"traceId\")[attribute].nunique().max() == 1\n",
    "\n",
    "\n",
    "selected_attributes = [attr for attr in attributes if is_trace_level(attr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get the durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_duration\"] = (data[\"end\"] - data[\"start\"]).dt.total_seconds().astype(int)\n",
    "\n",
    "data[\"activity_durations\"] = data.groupby(\"traceId\")[\"activity_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "data[\"transition_duration\"] = (\n",
    "    (data.groupby(\"traceId\")[\"start\"].shift(-1) - data[\"end\"])\n",
    "    .dt.total_seconds()\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "data[\"transition_durations\"] = data.groupby(\"traceId\")[\"transition_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "trace_total_duration = (\n",
    "    data.groupby(\"traceId\")\n",
    "    .apply(lambda x: (x[\"end\"].max() - x[\"start\"].min()).total_seconds())\n",
    "    .reset_index(name=\"trace_total_duration\")\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.merge(data, trace_total_duration, on=\"traceId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the activities and get the activities list / Take only the first row and assert durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All durations are consistent.\n"
     ]
    }
   ],
   "source": [
    "activities = data[\"activity\"].unique().tolist()\n",
    "activity_to_index = {activity: i for i, activity in enumerate(activities)}\n",
    "data[\"activity\"] = data[\"activity\"].map(activity_to_index)\n",
    "\n",
    "\n",
    "def decode_activities(indices, index_to_activity):\n",
    "    return [index_to_activity[index] for index in indices]\n",
    "\n",
    "\n",
    "data[\"trace_activity_list\"] = data.groupby(\"traceId\")[\"activity\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "data = data.drop_duplicates(subset=\"traceId\")[\n",
    "    [\n",
    "        \"traceId\",\n",
    "        \"trace_activity_list\",\n",
    "        \"activity_durations\",\n",
    "        \"transition_durations\",\n",
    "        \"trace_total_duration\",\n",
    "    ]\n",
    "    + selected_attributes\n",
    "]\n",
    "\n",
    "\n",
    "def assert_duration_consistency(row):\n",
    "    activity_sum = sum(row[\"activity_durations\"])\n",
    "    transition_sum = sum(row[\"transition_durations\"])\n",
    "    total_duration = row[\"trace_total_duration\"]\n",
    "    assert (\n",
    "        activity_sum + transition_sum == total_duration\n",
    "    ), f\"Inconsistency found in trace {row['traceId']}: {activity_sum} (activities) + {transition_sum} (transitions) != {total_duration} (total)\"\n",
    "\n",
    "\n",
    "data.apply(assert_duration_consistency, axis=1)\n",
    "print(\"All durations are consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = data[selected_attributes].select_dtypes(include=[\"bool\"]).columns\n",
    "for col in boolean_columns:\n",
    "    data[col] = data[col].astype(int)\n",
    "\n",
    "initial_columns = data.columns.tolist()\n",
    "\n",
    "data = pd.get_dummies(\n",
    "    data,\n",
    "    columns=data[selected_attributes]\n",
    "    .select_dtypes(include=[\"object\", \"category\"])\n",
    "    .columns,\n",
    ")\n",
    "\n",
    "new_dummy_columns = list(set(data.columns) - set(initial_columns))\n",
    "transformed_columns = list(boolean_columns) + new_dummy_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate sequences and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def truncate_sequence_random(seq):\n",
    "    if len(seq) > 1:\n",
    "        trunc_point = np.random.randint(1, len(seq))\n",
    "        truncated = seq[:trunc_point]\n",
    "        remaining = seq[trunc_point:]\n",
    "    else:\n",
    "        truncated = seq\n",
    "        remaining = []\n",
    "        trunc_point = len(seq)\n",
    "    return truncated, remaining, trunc_point\n",
    "\n",
    "\n",
    "def truncate_list(lst, trunc_points, offset=0):\n",
    "    truncated = [\n",
    "        item[: truncation_point - offset]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    remaining = [\n",
    "        item[truncation_point - offset :]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    return truncated, remaining\n",
    "\n",
    "\n",
    "data[[\"truncated_activity_list\", \"remaining_activity_list\", \"trunc_point\"]] = (\n",
    "    data[\"trace_activity_list\"].apply(truncate_sequence_random).apply(pd.Series)\n",
    ")\n",
    "\n",
    "data[\"truncated_durations\"], data[\"remaining_durations\"] = truncate_list(\n",
    "    data[\"activity_durations\"], data[\"trunc_point\"]\n",
    ")\n",
    "data[\"truncated_transitions\"], data[\"remaining_transitions\"] = truncate_list(\n",
    "    data[\"transition_durations\"], data[\"trunc_point\"], offset=1\n",
    ")\n",
    "\n",
    "data[\"truncated_total_duration\"] = data[\"truncated_durations\"].apply(sum) + data[\n",
    "    \"truncated_transitions\"\n",
    "].apply(sum)\n",
    "data[\"remaining_total_duration\"] = data[\"remaining_durations\"].apply(sum) + data[\n",
    "    \"remaining_transitions\"\n",
    "].apply(sum)\n",
    "\n",
    "assert all(\n",
    "    data[\"truncated_total_duration\"] + data[\"remaining_total_duration\"]\n",
    "    == data[\"trace_total_duration\"]\n",
    ")\n",
    "\n",
    "max_sequence_length = max(data[\"trace_activity_list\"].apply(len))\n",
    "\n",
    "data[\"truncated_activity_list\"] = pad_sequences(\n",
    "    data[\"truncated_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()\n",
    "\n",
    "data[\"remaining_activity_list\"] = pad_sequences(\n",
    "    data[\"remaining_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting, reshaping and one-hot encoding - if no Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)\n",
    "\n",
    "train_df = train_df.sort_values(by=\"traceId\")\n",
    "val_df = val_df.sort_values(by=\"traceId\")\n",
    "test_df = test_df.sort_values(by=\"traceId\")\n",
    "\n",
    "X_train_features = train_df[transformed_columns].values\n",
    "X_val_features = val_df[transformed_columns].values\n",
    "X_test_features = test_df[transformed_columns].values\n",
    "\n",
    "X_train = np.array(train_df[\"truncated_activity_list\"].tolist())\n",
    "Y_train = np.array(train_df[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X_val = np.array(val_df[\"truncated_activity_list\"].tolist())\n",
    "Y_val = np.array(val_df[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X_test = np.array(test_df[\"truncated_activity_list\"].tolist())\n",
    "Y_test = np.array(test_df[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "Y_val = Y_val.reshape(Y_val.shape[0], Y_val.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], Y_test.shape[1], 1)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train_features = X_train_features.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_val_features = X_val_features.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test_features = X_test_features.astype(np.float32)\n",
    "\n",
    "Y_train_onehot = to_categorical(Y_train.squeeze(), num_classes=n_unique_activities)\n",
    "Y_val_onehot = to_categorical(Y_val.squeeze(), num_classes=n_unique_activities)\n",
    "Y_test_onehot = to_categorical(Y_test.squeeze(), num_classes=n_unique_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def mask_acc(y_true, y_pred):\n",
    "    mask = K.cast(K.max(y_true, axis=-1), K.floatx())\n",
    "\n",
    "    y_true_labels = K.cast(K.argmax(y_true, axis=-1), K.floatx())\n",
    "    y_pred_labels = K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "\n",
    "    non_zero_mask = K.cast(K.greater(y_true_labels, 0), K.floatx())\n",
    "\n",
    "    is_correct = (\n",
    "        K.cast(K.equal(y_true_labels, y_pred_labels), K.floatx()) * mask * non_zero_mask\n",
    "    )\n",
    "    total_correct = K.sum(is_correct)\n",
    "    total_values = K.sum(mask * non_zero_mask)\n",
    "\n",
    "    return total_correct / total_values\n",
    "\n",
    "\n",
    "def seq_acc(y_true, y_pred):\n",
    "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "    y_true_labels = K.argmax(y_true, axis=-1)\n",
    "\n",
    "    correct_preds = K.all(K.equal(y_true_labels, y_pred_labels), axis=-1)\n",
    "\n",
    "    accuracy = K.mean(correct_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Bidirectional,\n",
    "    RepeatVector,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "sequence_input = Input(shape=(X_train.shape[1],))\n",
    "embedded_sequences = Embedding(input_dim=n_unique_activities, output_dim=64)(\n",
    "    sequence_input\n",
    ")\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "lstm_out = Dropout(0.15)(lstm_out)  # Add dropout after LSTM\n",
    "\n",
    "feature_input = Input(shape=(X_train_features.shape[1],))\n",
    "dense_feature = Dense(64, activation=\"relu\")(feature_input)\n",
    "dense_feature = Dropout(0.15)(dense_feature)  # Add dropout after first Dense layer\n",
    "dense_feature = Dense(64, activation=\"relu\")(dense_feature)\n",
    "repeated_feature = RepeatVector(X_train.shape[1])(dense_feature)\n",
    "\n",
    "concatenated = Concatenate(axis=-1)([lstm_out, repeated_feature])\n",
    "combined_dense = Dense(64, activation=\"relu\")(concatenated)\n",
    "\n",
    "output = Dense(n_unique_activities, activation=\"softmax\")(combined_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     [X_train, X_train_features],\n",
    "#     Y_train_onehot,\n",
    "#     epochs=10,  # Set the number of epochs you want to train for\n",
    "#     batch_size=32,  # Set the batch size according to your preference\n",
    "#     validation_data=(\n",
    "#         [X_val, X_val_features],\n",
    "#         Y_val_onehot,\n",
    "#     ),  # If you have validation data\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history[\"mask_acc\"])\n",
    "# plt.plot(history.history[\"val_mask_acc\"])\n",
    "# plt.title(\"Model mask accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation accuracy values for seq_acc\n",
    "# plt.plot(history.history[\"seq_acc\"])\n",
    "# plt.plot(history.history[\"val_seq_acc\"])\n",
    "# plt.title(\"Model sequence accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history[\"loss\"])\n",
    "# plt.plot(history.history[\"val_loss\"])\n",
    "# plt.title(\"Model loss\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 27s]\n",
      "val_seq_acc: 0.8757716019948324\n",
      "\n",
      "Best val_seq_acc So Far: 0.878375768661499\n",
      "Total elapsed time: 00h 02m 09s\n",
      "Epoch 1/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1170 - mask_acc: 0.8126 - seq_acc: 0.8674 - val_loss: 0.1369 - val_mask_acc: 0.7934 - val_seq_acc: 0.8735\n",
      "Epoch 2/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1189 - mask_acc: 0.8093 - seq_acc: 0.8754 - val_loss: 0.1340 - val_mask_acc: 0.7932 - val_seq_acc: 0.8748\n",
      "Epoch 3/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1158 - mask_acc: 0.8169 - seq_acc: 0.8737 - val_loss: 0.1359 - val_mask_acc: 0.7910 - val_seq_acc: 0.8748\n",
      "Epoch 4/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1157 - mask_acc: 0.8102 - seq_acc: 0.8720 - val_loss: 0.1368 - val_mask_acc: 0.7914 - val_seq_acc: 0.8726\n",
      "Epoch 5/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1140 - mask_acc: 0.8186 - seq_acc: 0.8827 - val_loss: 0.1370 - val_mask_acc: 0.7940 - val_seq_acc: 0.8739\n",
      "Epoch 6/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1160 - mask_acc: 0.8119 - seq_acc: 0.8783 - val_loss: 0.1348 - val_mask_acc: 0.7914 - val_seq_acc: 0.8769\n",
      "Epoch 7/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1178 - mask_acc: 0.8072 - seq_acc: 0.8733 - val_loss: 0.1349 - val_mask_acc: 0.7928 - val_seq_acc: 0.8743\n",
      "Epoch 8/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1139 - mask_acc: 0.8141 - seq_acc: 0.8750 - val_loss: 0.1387 - val_mask_acc: 0.7919 - val_seq_acc: 0.8791\n",
      "Epoch 9/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1176 - mask_acc: 0.8103 - seq_acc: 0.8753 - val_loss: 0.1350 - val_mask_acc: 0.7921 - val_seq_acc: 0.8752\n",
      "Epoch 10/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1158 - mask_acc: 0.8104 - seq_acc: 0.8672 - val_loss: 0.1397 - val_mask_acc: 0.7910 - val_seq_acc: 0.8735\n",
      "Fold accuracy: 0.7909618020057678\n",
      "Epoch 1/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.1229 - mask_acc: 0.8083 - seq_acc: 0.8760 - val_loss: 0.1176 - val_mask_acc: 0.8093 - val_seq_acc: 0.8788\n",
      "Epoch 2/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1224 - mask_acc: 0.8046 - seq_acc: 0.8740 - val_loss: 0.1192 - val_mask_acc: 0.8088 - val_seq_acc: 0.8736\n",
      "Epoch 3/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1222 - mask_acc: 0.7995 - seq_acc: 0.8705 - val_loss: 0.1193 - val_mask_acc: 0.8077 - val_seq_acc: 0.8788\n",
      "Epoch 4/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.1133 - mask_acc: 0.8153 - seq_acc: 0.8807 - val_loss: 0.1207 - val_mask_acc: 0.8140 - val_seq_acc: 0.8688\n",
      "Epoch 5/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1217 - mask_acc: 0.8076 - seq_acc: 0.8696 - val_loss: 0.1219 - val_mask_acc: 0.8087 - val_seq_acc: 0.8779\n",
      "Epoch 6/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1167 - mask_acc: 0.8101 - seq_acc: 0.8727 - val_loss: 0.1201 - val_mask_acc: 0.8076 - val_seq_acc: 0.8736\n",
      "Epoch 7/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1199 - mask_acc: 0.8093 - seq_acc: 0.8699 - val_loss: 0.1222 - val_mask_acc: 0.8080 - val_seq_acc: 0.8740\n",
      "Epoch 8/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1251 - mask_acc: 0.7979 - seq_acc: 0.8654 - val_loss: 0.1219 - val_mask_acc: 0.8070 - val_seq_acc: 0.8796\n",
      "Epoch 9/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1171 - mask_acc: 0.8106 - seq_acc: 0.8748 - val_loss: 0.1219 - val_mask_acc: 0.8093 - val_seq_acc: 0.8740\n",
      "Epoch 10/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1164 - mask_acc: 0.8073 - seq_acc: 0.8715 - val_loss: 0.1231 - val_mask_acc: 0.8112 - val_seq_acc: 0.8718\n",
      "Fold accuracy: 0.8112344741821289\n",
      "Epoch 1/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1295 - mask_acc: 0.8020 - seq_acc: 0.8665 - val_loss: 0.1089 - val_mask_acc: 0.8112 - val_seq_acc: 0.8839\n",
      "Epoch 2/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1223 - mask_acc: 0.8062 - seq_acc: 0.8748 - val_loss: 0.1116 - val_mask_acc: 0.8152 - val_seq_acc: 0.8739\n",
      "Epoch 3/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1147 - mask_acc: 0.8150 - seq_acc: 0.8794 - val_loss: 0.1115 - val_mask_acc: 0.8095 - val_seq_acc: 0.8782\n",
      "Epoch 4/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1190 - mask_acc: 0.8091 - seq_acc: 0.8705 - val_loss: 0.1117 - val_mask_acc: 0.8123 - val_seq_acc: 0.8761\n",
      "Epoch 5/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1166 - mask_acc: 0.8127 - seq_acc: 0.8717 - val_loss: 0.1132 - val_mask_acc: 0.8124 - val_seq_acc: 0.8843\n",
      "Epoch 6/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1184 - mask_acc: 0.8048 - seq_acc: 0.8747 - val_loss: 0.1117 - val_mask_acc: 0.8117 - val_seq_acc: 0.8860\n",
      "Epoch 7/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1220 - mask_acc: 0.7989 - seq_acc: 0.8706 - val_loss: 0.1131 - val_mask_acc: 0.8112 - val_seq_acc: 0.8856\n",
      "Epoch 8/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1161 - mask_acc: 0.8087 - seq_acc: 0.8738 - val_loss: 0.1143 - val_mask_acc: 0.8115 - val_seq_acc: 0.8821\n",
      "Epoch 9/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1184 - mask_acc: 0.8030 - seq_acc: 0.8678 - val_loss: 0.1143 - val_mask_acc: 0.8117 - val_seq_acc: 0.8860\n",
      "Epoch 10/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1137 - mask_acc: 0.8152 - seq_acc: 0.8762 - val_loss: 0.1165 - val_mask_acc: 0.8106 - val_seq_acc: 0.8834\n",
      "Fold accuracy: 0.8106115460395813\n",
      "Epoch 1/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1200 - mask_acc: 0.8064 - seq_acc: 0.8704 - val_loss: 0.1104 - val_mask_acc: 0.8100 - val_seq_acc: 0.8820\n",
      "Epoch 2/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1172 - mask_acc: 0.8083 - seq_acc: 0.8711 - val_loss: 0.1110 - val_mask_acc: 0.8081 - val_seq_acc: 0.8872\n",
      "Epoch 3/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.1161 - mask_acc: 0.8114 - seq_acc: 0.8774 - val_loss: 0.1120 - val_mask_acc: 0.8142 - val_seq_acc: 0.8799\n",
      "Epoch 4/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.1116 - mask_acc: 0.8162 - seq_acc: 0.8781 - val_loss: 0.1110 - val_mask_acc: 0.8119 - val_seq_acc: 0.8799\n",
      "Epoch 5/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1147 - mask_acc: 0.8099 - seq_acc: 0.8749 - val_loss: 0.1137 - val_mask_acc: 0.8095 - val_seq_acc: 0.8772\n",
      "Epoch 6/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1164 - mask_acc: 0.8113 - seq_acc: 0.8730 - val_loss: 0.1149 - val_mask_acc: 0.8148 - val_seq_acc: 0.8759\n",
      "Epoch 7/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1153 - mask_acc: 0.8113 - seq_acc: 0.8685 - val_loss: 0.1150 - val_mask_acc: 0.8123 - val_seq_acc: 0.8781\n",
      "Epoch 8/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1148 - mask_acc: 0.8147 - seq_acc: 0.8724 - val_loss: 0.1156 - val_mask_acc: 0.8105 - val_seq_acc: 0.8820\n",
      "Epoch 9/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1181 - mask_acc: 0.8037 - seq_acc: 0.8699 - val_loss: 0.1139 - val_mask_acc: 0.8109 - val_seq_acc: 0.8824\n",
      "Epoch 10/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1134 - mask_acc: 0.8109 - seq_acc: 0.8773 - val_loss: 0.1186 - val_mask_acc: 0.8114 - val_seq_acc: 0.8794\n",
      "Fold accuracy: 0.8114001154899597\n",
      "Epoch 1/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.1199 - mask_acc: 0.8042 - seq_acc: 0.8709 - val_loss: 0.1138 - val_mask_acc: 0.8049 - val_seq_acc: 0.8764\n",
      "Epoch 2/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1144 - mask_acc: 0.8102 - seq_acc: 0.8709 - val_loss: 0.1170 - val_mask_acc: 0.8041 - val_seq_acc: 0.8760\n",
      "Epoch 3/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1180 - mask_acc: 0.8036 - seq_acc: 0.8694 - val_loss: 0.1182 - val_mask_acc: 0.8042 - val_seq_acc: 0.8790\n",
      "Epoch 4/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1166 - mask_acc: 0.8039 - seq_acc: 0.8680 - val_loss: 0.1171 - val_mask_acc: 0.8037 - val_seq_acc: 0.8829\n",
      "Epoch 5/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1111 - mask_acc: 0.8139 - seq_acc: 0.8777 - val_loss: 0.1193 - val_mask_acc: 0.8073 - val_seq_acc: 0.8734\n",
      "Epoch 6/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1136 - mask_acc: 0.8022 - seq_acc: 0.8654 - val_loss: 0.1205 - val_mask_acc: 0.7988 - val_seq_acc: 0.8807\n",
      "Epoch 7/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1141 - mask_acc: 0.8082 - seq_acc: 0.8690 - val_loss: 0.1188 - val_mask_acc: 0.8141 - val_seq_acc: 0.8729\n",
      "Epoch 8/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1117 - mask_acc: 0.8099 - seq_acc: 0.8662 - val_loss: 0.1214 - val_mask_acc: 0.8062 - val_seq_acc: 0.8755\n",
      "Epoch 9/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1094 - mask_acc: 0.8130 - seq_acc: 0.8755 - val_loss: 0.1202 - val_mask_acc: 0.8113 - val_seq_acc: 0.8695\n",
      "Epoch 10/10\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1096 - mask_acc: 0.8199 - seq_acc: 0.8738 - val_loss: 0.1301 - val_mask_acc: 0.7911 - val_seq_acc: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/05 10:31:13 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold accuracy: 0.7910639643669128\n",
      "Mean accuracy: 0.8030543804168702 ± 0.009835410615194889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/05 10:31:17 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\mycke\\AppData\\Local\\Temp\\tmpcw61si6w\\model, flavor: keras). Fall back to return ['keras==3.3.3']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tuner import RandomSearch, Objective\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = np.array(data[\"truncated_activity_list\"].tolist())\n",
    "X_features = data[transformed_columns].values\n",
    "Y = np.array(data[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "Y = Y.reshape(Y.shape[0], Y.shape[1], 1)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "X_features = X_features.astype(np.float32)\n",
    "\n",
    "Y_onehot = to_categorical(Y.squeeze(), num_classes=n_unique_activities)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Model(inputs=[sequence_input, feature_input], outputs=output)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n",
    "        ),\n",
    "        metrics=[mask_acc, seq_acc],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "objective = Objective(\"val_seq_acc\", direction=\"max\")\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=objective,\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory=\"hyperparameter_tuning\",\n",
    "    project_name=\"activity_prediction\",\n",
    ")\n",
    "\n",
    "\n",
    "split = next(kfold.split(X, Y_onehot))\n",
    "train, test = split\n",
    "\n",
    "tuner.search(\n",
    "    [X[train], X_features[train]],\n",
    "    Y_onehot[train],\n",
    "    epochs=3,\n",
    "    validation_data=([X[test], X_features[test]], Y_onehot[test]),\n",
    ")\n",
    "\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    for fold, (train, test) in enumerate(kfold.split(X, Y_onehot)):\n",
    "        model = build_model(best_hps)\n",
    "\n",
    "        history = model.fit(\n",
    "            [X[train], X_features[train]],\n",
    "            Y_onehot[train],\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            validation_data=([X[test], X_features[test]], Y_onehot[test]),\n",
    "        )\n",
    "\n",
    "        scores = model.evaluate([X[test], X_features[test]], Y_onehot[test], verbose=0)\n",
    "        print(f\"Fold accuracy: {scores[1]}\")\n",
    "        cvscores.append(scores[1])\n",
    "\n",
    "        train_accuracies.append(history.history[\"seq_acc\"])\n",
    "        val_accuracies.append(history.history[\"val_seq_acc\"])\n",
    "        train_losses.append(history.history[\"loss\"])\n",
    "        val_losses.append(history.history[\"val_loss\"])\n",
    "\n",
    "        mlflow.log_metric(f\"fold_{fold}_accuracy\", scores[1])\n",
    "        mlflow.log_params(best_hps.values)\n",
    "        mlflow.log_params({\"epochs\": 10, \"batch_size\": 32})\n",
    "\n",
    "    mean_accuracy = np.mean(cvscores)\n",
    "    std_accuracy = np.std(cvscores)\n",
    "    mlflow.log_metric(\"mean_accuracy\", mean_accuracy)\n",
    "    mlflow.log_metric(\"std_accuracy\", std_accuracy)\n",
    "\n",
    "    print(f\"Mean accuracy: {mean_accuracy} ± {std_accuracy}\")\n",
    "\n",
    "    mlflow.keras.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracies and losses\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training and validation accuracy for each fold\n",
    "for i in range(kfold.get_n_splits()):\n",
    "    plt.plot(train_accuracies[i], label=f'Training Accuracy Fold {i+1}', linestyle='--', marker='o')\n",
    "    plt.plot(val_accuracies[i], label=f'Validation Accuracy Fold {i+1}', linestyle='-', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Accuracy for Each Fold')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training and validation loss for each fold\n",
    "for i in range(kfold.get_n_splits()):\n",
    "    plt.plot(train_losses[i], label=f'Training Loss Fold {i+1}', linestyle='--', marker='o')\n",
    "    plt.plot(val_losses[i], label=f'Validation Loss Fold {i+1}', linestyle='-', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss for Each Fold')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_sequences1 = model.predict([X_train, X_train_features])\n",
    "# predicted_sequences2 = model.predict([X_test, X_test_features])\n",
    "\n",
    "# predicted_activity_indices1 = [np.argmax(seq, axis=-1) for seq in predicted_sequences1]\n",
    "# predicted_activity_indices2 = [np.argmax(seq, axis=-1) for seq in predicted_sequences2]\n",
    "\n",
    "# train_df[\"predicted_sequence\"] = predicted_activity_indices1\n",
    "# test_df[\"predicted_sequence\"] = predicted_activity_indices2\n",
    "\n",
    "# combined_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "# combined_df = combined_df[[\"traceId\", \"predicted_sequence\"]].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
